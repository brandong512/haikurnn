{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "# tf_session = tf.compat.v1.Session()\n",
    "tf_session = tf.Session()\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "K.set_session(tf_session)\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,  CSVLogger\n",
    "from tensorflow.keras.layers import Add, Dense, Input, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "# import joblib\n",
    "\n",
    "\n",
    "# Local library with model definitions for training and generating\n",
    "from models import Generator, create_training_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "# Percent of samples to use for training, might be necessary if you're running out of memory\n",
    "sample_size = 1\n",
    "\n",
    "# The latent dimension of the LSTM\n",
    "latent_dim = 2048\n",
    "\n",
    "# Number of epochs to train for\n",
    "epochs = 20\n",
    "\n",
    "root_path = Path('../../..')\n",
    "input_path = root_path / 'input'\n",
    "poem_path = input_path / 'poems'\n",
    "haiku_path = poem_path / 'haikus.csv'\n",
    "\n",
    "name = 'all_data_test_2'\n",
    "output_dir = Path('output_%s' % name)\n",
    "output_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>source</th>\n",
       "      <th>0_syllables</th>\n",
       "      <th>1_syllables</th>\n",
       "      <th>2_syllables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7972</th>\n",
       "      <td>it isn't for anything a poem is country</td>\n",
       "      <td>and it needs you to keep walking it and i walk...</td>\n",
       "      <td>and smelling the paddocky wind and feeling the...</td>\n",
       "      <td>img2poems</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7713</th>\n",
       "      <td>the seer leaves the king alone in his throne room</td>\n",
       "      <td>and starts walking to china kicking up gravel</td>\n",
       "      <td>hurrying to find the next king</td>\n",
       "      <td>img2poems</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118126</th>\n",
       "      <td>Never thought I'd have</td>\n",
       "      <td>to think about moving on</td>\n",
       "      <td>but I'm doing it</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7478</th>\n",
       "      <td>this should be memorable</td>\n",
       "      <td>legs whited out</td>\n",
       "      <td>the runners advance</td>\n",
       "      <td>img2poems</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132760</th>\n",
       "      <td>Waking up on a</td>\n",
       "      <td>Friday is hard when you have</td>\n",
       "      <td>work on the weekend</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10770</th>\n",
       "      <td>a sensitive plant in a garden grew</td>\n",
       "      <td>and the young winds fed it with silver dew</td>\n",
       "      <td>and it opened its fan-like leaves to the light</td>\n",
       "      <td>img2poems</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82056</th>\n",
       "      <td>When someone smells nice</td>\n",
       "      <td>it automatically makes</td>\n",
       "      <td>them more attractive</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83199</th>\n",
       "      <td>Do you have any</td>\n",
       "      <td>questions for me Me ya'll have</td>\n",
       "      <td>pizza in china</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104333</th>\n",
       "      <td>You can never go</td>\n",
       "      <td>back after seeing The Last</td>\n",
       "      <td>Jedi But I'll try</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49994</th>\n",
       "      <td>Good morning How are</td>\n",
       "      <td>you today I hope you have</td>\n",
       "      <td>an amazing day</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143137 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0  \\\n",
       "7972              it isn't for anything a poem is country   \n",
       "7713    the seer leaves the king alone in his throne room   \n",
       "118126                             Never thought I'd have   \n",
       "7478                             this should be memorable   \n",
       "132760                                     Waking up on a   \n",
       "...                                                   ...   \n",
       "10770                  a sensitive plant in a garden grew   \n",
       "82056                            When someone smells nice   \n",
       "83199                                     Do you have any   \n",
       "104333                                   You can never go   \n",
       "49994                                Good morning How are   \n",
       "\n",
       "                                                        1  \\\n",
       "7972    and it needs you to keep walking it and i walk...   \n",
       "7713        and starts walking to china kicking up gravel   \n",
       "118126                           to think about moving on   \n",
       "7478                                      legs whited out   \n",
       "132760                       Friday is hard when you have   \n",
       "...                                                   ...   \n",
       "10770          and the young winds fed it with silver dew   \n",
       "82056                              it automatically makes   \n",
       "83199                      questions for me Me ya'll have   \n",
       "104333                         back after seeing The Last   \n",
       "49994                           you today I hope you have   \n",
       "\n",
       "                                                        2     source  \\\n",
       "7972    and smelling the paddocky wind and feeling the...  img2poems   \n",
       "7713                       hurrying to find the next king  img2poems   \n",
       "118126                                   but I'm doing it     twaiku   \n",
       "7478                                  the runners advance  img2poems   \n",
       "132760                                work on the weekend     twaiku   \n",
       "...                                                   ...        ...   \n",
       "10770      and it opened its fan-like leaves to the light  img2poems   \n",
       "82056                                them more attractive     twaiku   \n",
       "83199                                      pizza in china     twaiku   \n",
       "104333                                  Jedi But I'll try     twaiku   \n",
       "49994                                      an amazing day     twaiku   \n",
       "\n",
       "       0_syllables 1_syllables 2_syllables  \n",
       "7972            13          22          17  \n",
       "7713            11          12           8  \n",
       "118126           5           7           5  \n",
       "7478             7           4           5  \n",
       "132760           5           7           5  \n",
       "...            ...         ...         ...  \n",
       "10770           10          10          10  \n",
       "82056            5           7           5  \n",
       "83199            5           7           5  \n",
       "104333           5           7           5  \n",
       "49994            5           7           5  \n",
       "\n",
       "[143137 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(str(haiku_path))\n",
    "df = df.sample(frac=sample_size)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Input for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>0_syllables</th>\n",
       "      <th>1_syllables</th>\n",
       "      <th>2_syllables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Memorial Day --</td>\n",
       "      <td>a shadow for each</td>\n",
       "      <td>white cross</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring rain -</td>\n",
       "      <td>as the doctor speaks</td>\n",
       "      <td>i think of lilacs</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring rain -</td>\n",
       "      <td>as the doctor speaks</td>\n",
       "      <td>i think of lilacs</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring moonset --</td>\n",
       "      <td>a rice ball for</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring moonset --</td>\n",
       "      <td>a rice ball for</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143132</th>\n",
       "      <td>I'm not asking did</td>\n",
       "      <td>you say it nor clarify</td>\n",
       "      <td>what you said neither</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143133</th>\n",
       "      <td>You are truly a</td>\n",
       "      <td>moron or a liar I'm</td>\n",
       "      <td>inclined to think both</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143134</th>\n",
       "      <td>Ain't no selfie on</td>\n",
       "      <td>this earth that's gonna make me</td>\n",
       "      <td>like Theresa May</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143135</th>\n",
       "      <td>is doing a great</td>\n",
       "      <td>job turning Independents</td>\n",
       "      <td>into Democrats</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143136</th>\n",
       "      <td>Wanted to send a</td>\n",
       "      <td>quick follow up on if the</td>\n",
       "      <td>blood is loud Talk soon</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>172759 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0                                 1  \\\n",
       "0          Memorial Day --                 a shadow for each   \n",
       "1            spring rain -              as the doctor speaks   \n",
       "1            spring rain -              as the doctor speaks   \n",
       "2        spring moonset --                   a rice ball for   \n",
       "2        spring moonset --                   a rice ball for   \n",
       "...                    ...                               ...   \n",
       "143132  I'm not asking did            you say it nor clarify   \n",
       "143133     You are truly a               moron or a liar I'm   \n",
       "143134  Ain't no selfie on   this earth that's gonna make me   \n",
       "143135    is doing a great          job turning Independents   \n",
       "143136    Wanted to send a         quick follow up on if the   \n",
       "\n",
       "                              2 0_syllables 1_syllables 2_syllables  \n",
       "0                   white cross           5           5           2  \n",
       "1             i think of lilacs           2           5           5  \n",
       "1             i think of lilacs           3           5           5  \n",
       "2                     breakfast           3           4           2  \n",
       "2                     breakfast           4           4           2  \n",
       "...                         ...         ...         ...         ...  \n",
       "143132    what you said neither           5           7           5  \n",
       "143133   inclined to think both           5           7           5  \n",
       "143134         like Theresa May           5           7           5  \n",
       "143135           into Democrats           5           7           5  \n",
       "143136  blood is loud Talk soon           5           7           5  \n",
       "\n",
       "[172759 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate lines with ambiguous syllable counts\n",
    "# (syllable counts where there is a comma because\n",
    "# multiple pronounciations are acceptable)\n",
    "\n",
    "lines = set([0, 1, 2])\n",
    "\n",
    "for i in range(3):\n",
    "    lines.remove(i)\n",
    "    df = df[[\n",
    "        '0', '1', '2',\n",
    "        #'1_syllables', '2_syllables'\n",
    "    ] + ['%s_syllables' % j for j in lines]].join(\n",
    "        df['%s_syllables' % i].str.split(\n",
    "            ',', expand=True\n",
    "        ).stack(-1).reset_index(\n",
    "            level=1, drop=True\n",
    "        ).rename('%s_syllables' % i)\n",
    "    ).drop_duplicates()\n",
    "    lines.add(i)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>0_syllables</th>\n",
       "      <th>1_syllables</th>\n",
       "      <th>2_syllables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Memorial Day --</td>\n",
       "      <td>a shadow for each</td>\n",
       "      <td>white cross</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring rain -</td>\n",
       "      <td>as the doctor speaks</td>\n",
       "      <td>i think of lilacs</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring rain -</td>\n",
       "      <td>as the doctor speaks</td>\n",
       "      <td>i think of lilacs</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring moonset --</td>\n",
       "      <td>a rice ball for</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring moonset --</td>\n",
       "      <td>a rice ball for</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143132</th>\n",
       "      <td>I'm not asking did</td>\n",
       "      <td>you say it nor clarify</td>\n",
       "      <td>what you said neither</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143133</th>\n",
       "      <td>You are truly a</td>\n",
       "      <td>moron or a liar I'm</td>\n",
       "      <td>inclined to think both</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143134</th>\n",
       "      <td>Ain't no selfie on</td>\n",
       "      <td>this earth that's gonna make me</td>\n",
       "      <td>like Theresa May</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143135</th>\n",
       "      <td>is doing a great</td>\n",
       "      <td>job turning Independents</td>\n",
       "      <td>into Democrats</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143136</th>\n",
       "      <td>Wanted to send a</td>\n",
       "      <td>quick follow up on if the</td>\n",
       "      <td>blood is loud Talk soon</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170155 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0                                 1  \\\n",
       "0          Memorial Day --                 a shadow for each   \n",
       "1            spring rain -              as the doctor speaks   \n",
       "1            spring rain -              as the doctor speaks   \n",
       "2        spring moonset --                   a rice ball for   \n",
       "2        spring moonset --                   a rice ball for   \n",
       "...                    ...                               ...   \n",
       "143132  I'm not asking did            you say it nor clarify   \n",
       "143133     You are truly a               moron or a liar I'm   \n",
       "143134  Ain't no selfie on   this earth that's gonna make me   \n",
       "143135    is doing a great          job turning Independents   \n",
       "143136    Wanted to send a         quick follow up on if the   \n",
       "\n",
       "                              2 0_syllables 1_syllables 2_syllables  \n",
       "0                   white cross           5           5           2  \n",
       "1             i think of lilacs           2           5           5  \n",
       "1             i think of lilacs           3           5           5  \n",
       "2                     breakfast           3           4           2  \n",
       "2                     breakfast           4           4           2  \n",
       "...                         ...         ...         ...         ...  \n",
       "143132    what you said neither           5           7           5  \n",
       "143133   inclined to think both           5           7           5  \n",
       "143134         like Theresa May           5           7           5  \n",
       "143135           into Democrats           5           7           5  \n",
       "143136  blood is loud Talk soon           5           7           5  \n",
       "\n",
       "[170155 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop samples that are longer that the 99th percentile of length\n",
    "\n",
    "max_line_length = int(max([df['%s' % i].str.len().quantile(.99) for i in range(3)]))\n",
    "df = df[\n",
    "    (df['0'].str.len() <= max_line_length) & \n",
    "    (df['1'].str.len() <= max_line_length) & \n",
    "    (df['2'].str.len() <= max_line_length)\n",
    "].copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>0_syllables</th>\n",
       "      <th>1_syllables</th>\n",
       "      <th>2_syllables</th>\n",
       "      <th>0_in</th>\n",
       "      <th>0_out</th>\n",
       "      <th>1_in</th>\n",
       "      <th>1_out</th>\n",
       "      <th>2_in</th>\n",
       "      <th>2_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Memorial Day --</td>\n",
       "      <td>a shadow for each</td>\n",
       "      <td>white cross</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>MMemorial Day --\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>Memorial Day --\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>aa shadow for each\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>a shadow for each\\nw\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>wwhite cross\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>white cross\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring rain -</td>\n",
       "      <td>as the doctor speaks</td>\n",
       "      <td>i think of lilacs</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>sspring rain -\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>spring rain -\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>aas the doctor speaks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>as the doctor speaks\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>ii think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>i think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring rain -</td>\n",
       "      <td>as the doctor speaks</td>\n",
       "      <td>i think of lilacs</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>sspring rain -\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>spring rain -\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>aas the doctor speaks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>as the doctor speaks\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>ii think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>i think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring moonset --</td>\n",
       "      <td>a rice ball for</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>sspring moonset --\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>spring moonset --\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>aa rice ball for\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>a rice ball for\\nb\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>bbreakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>breakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring moonset --</td>\n",
       "      <td>a rice ball for</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>sspring moonset --\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>spring moonset --\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>aa rice ball for\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>a rice ball for\\nb\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>bbreakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>breakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143132</th>\n",
       "      <td>I'm not asking did</td>\n",
       "      <td>you say it nor clarify</td>\n",
       "      <td>what you said neither</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>II'm not asking did\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>I'm not asking did\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>you say it nor clarify\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>you say it nor clarify\\nw\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>wwhat you said neither\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>what you said neither\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143133</th>\n",
       "      <td>You are truly a</td>\n",
       "      <td>moron or a liar I'm</td>\n",
       "      <td>inclined to think both</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>YYou are truly a\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>You are truly a\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>moron or a liar I'm\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>moron or a liar I'm\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>iinclined to think both\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>inclined to think both\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143134</th>\n",
       "      <td>Ain't no selfie on</td>\n",
       "      <td>this earth that's gonna make me</td>\n",
       "      <td>like Theresa May</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>AAin't no selfie on\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>Ain't no selfie on\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>this earth that's gonna make me\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>this earth that's gonna make me\\nl\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>llike Theresa May\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>like Theresa May\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143135</th>\n",
       "      <td>is doing a great</td>\n",
       "      <td>job turning Independents</td>\n",
       "      <td>into Democrats</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>iis doing a great\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>is doing a great\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>job turning Independents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>job turning Independents\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>iinto Democrats\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>into Democrats\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143136</th>\n",
       "      <td>Wanted to send a</td>\n",
       "      <td>quick follow up on if the</td>\n",
       "      <td>blood is loud Talk soon</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>WWanted to send a\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>Wanted to send a\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>quick follow up on if the\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>quick follow up on if the\\nb\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>bblood is loud Talk soon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>blood is loud Talk soon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170155 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0                                 1  \\\n",
       "0          Memorial Day --                 a shadow for each   \n",
       "1            spring rain -              as the doctor speaks   \n",
       "1            spring rain -              as the doctor speaks   \n",
       "2        spring moonset --                   a rice ball for   \n",
       "2        spring moonset --                   a rice ball for   \n",
       "...                    ...                               ...   \n",
       "143132  I'm not asking did            you say it nor clarify   \n",
       "143133     You are truly a               moron or a liar I'm   \n",
       "143134  Ain't no selfie on   this earth that's gonna make me   \n",
       "143135    is doing a great          job turning Independents   \n",
       "143136    Wanted to send a         quick follow up on if the   \n",
       "\n",
       "                              2 0_syllables 1_syllables 2_syllables  \\\n",
       "0                   white cross           5           5           2   \n",
       "1             i think of lilacs           2           5           5   \n",
       "1             i think of lilacs           3           5           5   \n",
       "2                     breakfast           3           4           2   \n",
       "2                     breakfast           4           4           2   \n",
       "...                         ...         ...         ...         ...   \n",
       "143132    what you said neither           5           7           5   \n",
       "143133   inclined to think both           5           7           5   \n",
       "143134         like Theresa May           5           7           5   \n",
       "143135           into Democrats           5           7           5   \n",
       "143136  blood is loud Talk soon           5           7           5   \n",
       "\n",
       "                                                     0_in  \\\n",
       "0       MMemorial Day --\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       sspring rain -\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       sspring rain -\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       sspring moonset --\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       sspring moonset --\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "...                                                   ...   \n",
       "143132  II'm not asking did\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143133  YYou are truly a\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143134  AAin't no selfie on\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143135  iis doing a great\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143136  WWanted to send a\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "\n",
       "                                                    0_out  \\\n",
       "0       Memorial Day --\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       spring rain -\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       spring rain -\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       spring moonset --\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       spring moonset --\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "...                                                   ...   \n",
       "143132  I'm not asking did\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143133  You are truly a\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143134  Ain't no selfie on\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143135  is doing a great\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143136  Wanted to send a\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "\n",
       "                                                     1_in  \\\n",
       "0       aa shadow for each\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       aas the doctor speaks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "1       aas the doctor speaks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "2       aa rice ball for\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       aa rice ball for\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "...                                                   ...   \n",
       "143132    you say it nor clarify\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143133    moron or a liar I'm\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143134    this earth that's gonna make me\\n\\n\\n\\n\\n\\n\\...   \n",
       "143135    job turning Independents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143136    quick follow up on if the\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "\n",
       "                                                    1_out  \\\n",
       "0       a shadow for each\\nw\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       as the doctor speaks\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "1       as the doctor speaks\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "2       a rice ball for\\nb\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       a rice ball for\\nb\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "...                                                   ...   \n",
       "143132   you say it nor clarify\\nw\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143133   moron or a liar I'm\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143134   this earth that's gonna make me\\nl\\n\\n\\n\\n\\n\\...   \n",
       "143135   job turning Independents\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143136   quick follow up on if the\\nb\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "\n",
       "                                                     2_in  \\\n",
       "0       wwhite cross\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       ii think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       ii think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       bbreakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       bbreakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "...                                                   ...   \n",
       "143132  wwhat you said neither\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143133  iinclined to think both\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143134  llike Theresa May\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143135  iinto Democrats\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143136  bblood is loud Talk soon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "\n",
       "                                                    2_out  \n",
       "0       white cross\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "1       i think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "1       i think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "2       breakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "2       breakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "...                                                   ...  \n",
       "143132  what you said neither\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "143133  inclined to think both\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "143134  like Theresa May\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "143135  into Democrats\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "143136  blood is loud Talk soon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "\n",
       "[170155 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad the lines to the max line length with new lines\n",
    "for i in range(3):\n",
    "    # For input, duplicate the first character\n",
    "    # TODO - Why?\n",
    "    df['%s_in' % i] = (df[str(i)].str[0] + df[str(i)]).str.pad(max_line_length+2, 'right', '\\n')\n",
    "    \n",
    "    # \n",
    "    #df['%s_out' % i] = df[str(i)].str.pad(max_line_len, 'right', '\\n') + ('\\n' if i == 2 else df[str(i+1)].str[0])\n",
    "    \n",
    "    # TODO - trying to add the next line's first character before the line breaks\n",
    "    if i == 2: # If it's the last line\n",
    "        df['%s_out' % i] = df[str(i)].str.pad(max_line_length+2, 'right', '\\n')\n",
    "    else: \n",
    "        # If it's the first or second line, add the first character of the next line to the end of this line.\n",
    "        # This helps with training so that the next RNN has a better chance of getting the first character right.\n",
    "        df['%s_out' % i] = (df[str(i)] + '\\n' + df[str(i+1)].str[0]).str.pad(max_line_length+2, 'right', '\\n')\n",
    "    \n",
    "max_line_length += 2\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df[['0_in', '1_in', '2_in']].values\n",
    "\n",
    "tokenizer = Tokenizer(filters='', char_level=True)\n",
    "tokenizer.fit_on_texts(inputs.flatten())\n",
    "n_tokens = len(tokenizer.word_counts) + 1\n",
    "\n",
    "# X is the input for each line in sequences of one-hot-encoded values\n",
    "X = to_categorical([\n",
    "    tokenizer.texts_to_sequences(inputs[:,i]) for i in range(3)\n",
    "], num_classes=n_tokens)\n",
    "\n",
    "outputs = df[['0_out', '1_out', '2_out']].values\n",
    "\n",
    "# Y is the output for each line in sequences of one-hot-encoded values\n",
    "Y = to_categorical([\n",
    "    tokenizer.texts_to_sequences(outputs[:,i]) for i in range(3)\n",
    "], num_classes=n_tokens)\n",
    "\n",
    "# X_syllables is the count of syllables for each line\n",
    "X_syllables = df[['0_syllables', '1_syllables', '2_syllables']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw shape:  (170155,)\n",
      "shape after text-->sequence:  170155\n"
     ]
    }
   ],
   "source": [
    "# Testing transformations made on text corpus\n",
    "\n",
    "# inputs.shape\n",
    "# inputs.flatten().shape --> turns into 1d array\n",
    "\n",
    "X.shape\n",
    "# inputs\n",
    "print(\"raw shape: \", inputs[:, 0].shape)\n",
    "\n",
    "print(\"shape after text-->sequence: \", len(tokenizer.texts_to_sequences(inputs[:,0])))\n",
    "# print(\"shape in np.to_cag: \", np_utils.to_categorical([\n",
    "#     tokenizer.texts_to_sequences(inputs[:,0]),\n",
    "#     tokenizer.texts_to_sequences(inputs[:,1]),\n",
    "#     # tokenizer.texts_to_sequences(inputs[:,2])\n",
    "# ]).shape)\n",
    "# tokenizer.texts_to_sequences(inputs[:,0])\n",
    "# print(inputs[0, 0])\n",
    "#? np_utils.to_categorical(tokenizer.texts_to_sequences(inputs[:,0]))[:, :, 0].shape\n",
    "\n",
    "# inputs[:, column is the haiku line]\n",
    "\n",
    "#! shape of X (row #, haiku line number, ???, ???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output_all_data_test_2\\\\metadata.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump([latent_dim, n_tokens, max_line_length, tokenizer], str(output_dir / 'metadata.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_model, lstm, lines, inputs, outputs = create_training_model(latent_dim, n_tokens)\n",
    "\n",
    "filepath = str(output_dir / (\"%s-{epoch:02d}-{loss:.2f}-{val_loss:.2f}.hdf5\" % latent_dim))\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "csv_logger = CSVLogger(str(output_dir / 'training_log.csv'), append=True, separator=',')\n",
    "\n",
    "callbacks_list = [checkpoint, csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 1., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 1., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 1., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n",
    "X_syllables\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 153139 samples, validate on 17016 samples\n",
      "Epoch 1/20\n",
      "  1600/153139 [..............................] - ETA: 24:57:38 - loss: 7.0011 - output_line_0_loss: 2.1301 - output_line_1_loss: 2.6852 - output_line_2_loss: 2.1858"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-dfeb6730b5c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_syllables\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_syllables\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m ], [Y[0], Y[1], Y[2]], epochs=epochs, validation_split=.1)\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\brand\\Desktop\\Github Repos\\NLP\\haikurnn\\haiku-3.6\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\Users\\brand\\Desktop\\Github Repos\\NLP\\haikurnn\\haiku-3.6\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\brand\\Desktop\\Github Repos\\NLP\\haikurnn\\haiku-3.6\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 2986\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2988\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\brand\\Desktop\\Github Repos\\NLP\\haikurnn\\haiku-3.6\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_model.fit([\n",
    "    X[0], X_syllables[:,0], \n",
    "    X[1], X_syllables[:,1], \n",
    "    X[2], X_syllables[:,2]\n",
    "], [Y[0], Y[1], Y[2]], epochs=epochs, validation_split=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(lstm, lines, tf_session, tokenizer, n_tokens, max_line_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-ff420846a678>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_haiku\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "# import importlib\n",
    "# importlib.reload(models)\n",
    "generator.generate_haiku()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
