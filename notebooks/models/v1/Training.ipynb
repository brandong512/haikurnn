{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "# tf_session = tf.compat.v1.Session()\n",
    "tf_session = tf.Session()\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "K.set_session(tf_session)\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,  CSVLogger\n",
    "from tensorflow.keras.layers import Add, Dense, Input, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "# import joblib\n",
    "\n",
    "\n",
    "# Local library with model definitions for training and generating\n",
    "from models import Generator, create_training_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22. 28.]\n",
      " [49. 64.]]\n"
     ]
    }
   ],
   "source": [
    "# Creates a graph.\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "# Percent of samples to use for training, might be necessary if you're running out of memory\n",
    "sample_size = 1\n",
    "\n",
    "# The latent dimension of the LSTM\n",
    "latent_dim = 2048\n",
    "\n",
    "# Number of epochs to train for\n",
    "epochs = 20\n",
    "\n",
    "root_path = Path('../../..')\n",
    "input_path = root_path / 'input'\n",
    "poem_path = input_path / 'poems'\n",
    "haiku_path = poem_path / 'haikus.csv'\n",
    "\n",
    "name = 'all_data_test_2'\n",
    "output_dir = Path('output_%s' % name)\n",
    "output_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>source</th>\n",
       "      <th>0_syllables</th>\n",
       "      <th>1_syllables</th>\n",
       "      <th>2_syllables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>130322</th>\n",
       "      <td>my printer is such</td>\n",
       "      <td>a piece of shit why do we</td>\n",
       "      <td>even still have it</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23213</th>\n",
       "      <td>red berries</td>\n",
       "      <td>at the tip of each branch</td>\n",
       "      <td>the setting sun</td>\n",
       "      <td>sballas</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61691</th>\n",
       "      <td>So that explains why</td>\n",
       "      <td>the Burning Bush didn't have</td>\n",
       "      <td>limbs No fire arms</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>6,7</td>\n",
       "      <td>4,5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4905</th>\n",
       "      <td>my soul lashes out at all the meanness</td>\n",
       "      <td>bred in soullessness</td>\n",
       "      <td>few care who they hurt reason for sadness</td>\n",
       "      <td>img2poems</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4014</th>\n",
       "      <td>church graveyard</td>\n",
       "      <td>a cloud of crows hover</td>\n",
       "      <td>over stone angels</td>\n",
       "      <td>tempslibres</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>July Fourth fireworks---</td>\n",
       "      <td>I stand next to</td>\n",
       "      <td>an Iraqi refugee</td>\n",
       "      <td>tempslibres</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51875</th>\n",
       "      <td>I hope someone buys</td>\n",
       "      <td>me sunflowers tomorrow</td>\n",
       "      <td>I'll be so happy</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78785</th>\n",
       "      <td>Been playing a Red</td>\n",
       "      <td>Faction Guerilla It's good</td>\n",
       "      <td>Good for what ails ya</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42262</th>\n",
       "      <td>Anyone have snow</td>\n",
       "      <td>gear I can borrow All I</td>\n",
       "      <td>need is pants and boots</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29420</th>\n",
       "      <td>\" The Great Adventure of</td>\n",
       "      <td>Max Breuck: Stanza headings were originally</td>\n",
       "      <td>Roman Numerals.</td>\n",
       "      <td>gutenberg</td>\n",
       "      <td>6</td>\n",
       "      <td>11,12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143137 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             0  \\\n",
       "130322                      my printer is such   \n",
       "23213                              red berries   \n",
       "61691                     So that explains why   \n",
       "4905    my soul lashes out at all the meanness   \n",
       "4014                          church graveyard   \n",
       "...                                        ...   \n",
       "1814                  July Fourth fireworks---   \n",
       "51875                      I hope someone buys   \n",
       "78785                       Been playing a Red   \n",
       "42262                         Anyone have snow   \n",
       "29420                 \" The Great Adventure of   \n",
       "\n",
       "                                                  1  \\\n",
       "130322                    a piece of shit why do we   \n",
       "23213                     at the tip of each branch   \n",
       "61691                  the Burning Bush didn't have   \n",
       "4905                           bred in soullessness   \n",
       "4014                         a cloud of crows hover   \n",
       "...                                             ...   \n",
       "1814                                I stand next to   \n",
       "51875                        me sunflowers tomorrow   \n",
       "78785                    Faction Guerilla It's good   \n",
       "42262                       gear I can borrow All I   \n",
       "29420   Max Breuck: Stanza headings were originally   \n",
       "\n",
       "                                                2       source 0_syllables  \\\n",
       "130322                         even still have it       twaiku           5   \n",
       "23213                             the setting sun      sballas           3   \n",
       "61691                          limbs No fire arms       twaiku           5   \n",
       "4905    few care who they hurt reason for sadness    img2poems          10   \n",
       "4014                            over stone angels  tempslibres           3   \n",
       "...                                           ...          ...         ...   \n",
       "1814                            an Iraqi refugee   tempslibres           5   \n",
       "51875                            I'll be so happy       twaiku           5   \n",
       "78785                       Good for what ails ya       twaiku           5   \n",
       "42262                     need is pants and boots       twaiku           5   \n",
       "29420                             Roman Numerals.    gutenberg           6   \n",
       "\n",
       "       1_syllables 2_syllables  \n",
       "130322           7           5  \n",
       "23213            6           4  \n",
       "61691          6,7         4,5  \n",
       "4905             5          10  \n",
       "4014             6           5  \n",
       "...            ...         ...  \n",
       "1814             4           7  \n",
       "51875            7           5  \n",
       "78785            7           5  \n",
       "42262            7           5  \n",
       "29420        11,12           5  \n",
       "\n",
       "[143137 rows x 7 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(str(haiku_path))\n",
    "df = df.sample(frac=sample_size)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Input for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>0_syllables</th>\n",
       "      <th>1_syllables</th>\n",
       "      <th>2_syllables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Memorial Day --</td>\n",
       "      <td>a shadow for each</td>\n",
       "      <td>white cross</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring rain -</td>\n",
       "      <td>as the doctor speaks</td>\n",
       "      <td>i think of lilacs</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring rain -</td>\n",
       "      <td>as the doctor speaks</td>\n",
       "      <td>i think of lilacs</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring moonset --</td>\n",
       "      <td>a rice ball for</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring moonset --</td>\n",
       "      <td>a rice ball for</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sunny afternoon</td>\n",
       "      <td>an old man lingers</td>\n",
       "      <td>near the mailbox</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cinco de mayo</td>\n",
       "      <td>horses roll</td>\n",
       "      <td>in the shallows</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quitting time</td>\n",
       "      <td>the smell of rain</td>\n",
       "      <td>in the lobby</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>waves</td>\n",
       "      <td>slowly cresting towards shore</td>\n",
       "      <td>a faint moon</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>waves</td>\n",
       "      <td>slowly cresting towards shore</td>\n",
       "      <td>a faint moon</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0                              1                  2  \\\n",
       "0    Memorial Day --              a shadow for each        white cross   \n",
       "1      spring rain -           as the doctor speaks  i think of lilacs   \n",
       "1      spring rain -           as the doctor speaks  i think of lilacs   \n",
       "2  spring moonset --                a rice ball for          breakfast   \n",
       "2  spring moonset --                a rice ball for          breakfast   \n",
       "3    sunny afternoon             an old man lingers   near the mailbox   \n",
       "4      cinco de mayo                    horses roll    in the shallows   \n",
       "5      quitting time              the smell of rain       in the lobby   \n",
       "6              waves  slowly cresting towards shore       a faint moon   \n",
       "6              waves  slowly cresting towards shore       a faint moon   \n",
       "\n",
       "  0_syllables 1_syllables 2_syllables  \n",
       "0           5           5           2  \n",
       "1           2           5           5  \n",
       "1           3           5           5  \n",
       "2           3           4           2  \n",
       "2           4           4           2  \n",
       "3           5           5           4  \n",
       "4           5           3           4  \n",
       "5           3           4           4  \n",
       "6           1           6           3  \n",
       "6           1           7           3  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate lines with ambiguous syllable counts\n",
    "# (syllable counts where there is a comma because\n",
    "# multiple pronounciations are acceptable)\n",
    "\n",
    "lines = set([0, 1, 2])\n",
    "\n",
    "for i in range(3):\n",
    "    lines.remove(i)\n",
    "    df = df[[\n",
    "        '0', '1', '2',\n",
    "        #'1_syllables', '2_syllables'\n",
    "    ] + ['%s_syllables' % j for j in lines]].join(\n",
    "        df['%s_syllables' % i].str.split(\n",
    "            ',', expand=True\n",
    "        ).stack(-1).reset_index(\n",
    "            level=1, drop=True\n",
    "        ).rename('%s_syllables' % i)\n",
    "    ).drop_duplicates()\n",
    "    lines.add(i)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>0_syllables</th>\n",
       "      <th>1_syllables</th>\n",
       "      <th>2_syllables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Memorial Day --</td>\n",
       "      <td>a shadow for each</td>\n",
       "      <td>white cross</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring rain -</td>\n",
       "      <td>as the doctor speaks</td>\n",
       "      <td>i think of lilacs</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring rain -</td>\n",
       "      <td>as the doctor speaks</td>\n",
       "      <td>i think of lilacs</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring moonset --</td>\n",
       "      <td>a rice ball for</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring moonset --</td>\n",
       "      <td>a rice ball for</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143132</th>\n",
       "      <td>I'm not asking did</td>\n",
       "      <td>you say it nor clarify</td>\n",
       "      <td>what you said neither</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143133</th>\n",
       "      <td>You are truly a</td>\n",
       "      <td>moron or a liar I'm</td>\n",
       "      <td>inclined to think both</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143134</th>\n",
       "      <td>Ain't no selfie on</td>\n",
       "      <td>this earth that's gonna make me</td>\n",
       "      <td>like Theresa May</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143135</th>\n",
       "      <td>is doing a great</td>\n",
       "      <td>job turning Independents</td>\n",
       "      <td>into Democrats</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143136</th>\n",
       "      <td>Wanted to send a</td>\n",
       "      <td>quick follow up on if the</td>\n",
       "      <td>blood is loud Talk soon</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170155 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0                                 1  \\\n",
       "0          Memorial Day --                 a shadow for each   \n",
       "1            spring rain -              as the doctor speaks   \n",
       "1            spring rain -              as the doctor speaks   \n",
       "2        spring moonset --                   a rice ball for   \n",
       "2        spring moonset --                   a rice ball for   \n",
       "...                    ...                               ...   \n",
       "143132  I'm not asking did            you say it nor clarify   \n",
       "143133     You are truly a               moron or a liar I'm   \n",
       "143134  Ain't no selfie on   this earth that's gonna make me   \n",
       "143135    is doing a great          job turning Independents   \n",
       "143136    Wanted to send a         quick follow up on if the   \n",
       "\n",
       "                              2 0_syllables 1_syllables 2_syllables  \n",
       "0                   white cross           5           5           2  \n",
       "1             i think of lilacs           2           5           5  \n",
       "1             i think of lilacs           3           5           5  \n",
       "2                     breakfast           3           4           2  \n",
       "2                     breakfast           4           4           2  \n",
       "...                         ...         ...         ...         ...  \n",
       "143132    what you said neither           5           7           5  \n",
       "143133   inclined to think both           5           7           5  \n",
       "143134         like Theresa May           5           7           5  \n",
       "143135           into Democrats           5           7           5  \n",
       "143136  blood is loud Talk soon           5           7           5  \n",
       "\n",
       "[170155 rows x 6 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop samples that are longer that the 99th percentile of length\n",
    "\n",
    "max_line_length = int(max([df['%s' % i].str.len().quantile(.99) for i in range(3)]))\n",
    "df = df[\n",
    "    (df['0'].str.len() <= max_line_length) & \n",
    "    (df['1'].str.len() <= max_line_length) & \n",
    "    (df['2'].str.len() <= max_line_length)\n",
    "].copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>0_syllables</th>\n",
       "      <th>1_syllables</th>\n",
       "      <th>2_syllables</th>\n",
       "      <th>0_in</th>\n",
       "      <th>0_out</th>\n",
       "      <th>1_in</th>\n",
       "      <th>1_out</th>\n",
       "      <th>2_in</th>\n",
       "      <th>2_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Memorial Day --</td>\n",
       "      <td>a shadow for each</td>\n",
       "      <td>white cross</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>MMemorial Day --\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>Memorial Day --\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>aa shadow for each\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>a shadow for each\\nw\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>wwhite cross\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>white cross\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring rain -</td>\n",
       "      <td>as the doctor speaks</td>\n",
       "      <td>i think of lilacs</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>sspring rain -\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>spring rain -\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>aas the doctor speaks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>as the doctor speaks\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>ii think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>i think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring rain -</td>\n",
       "      <td>as the doctor speaks</td>\n",
       "      <td>i think of lilacs</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>sspring rain -\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>spring rain -\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>aas the doctor speaks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>as the doctor speaks\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>ii think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>i think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring moonset --</td>\n",
       "      <td>a rice ball for</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>sspring moonset --\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>spring moonset --\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>aa rice ball for\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>a rice ball for\\nb\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>bbreakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>breakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring moonset --</td>\n",
       "      <td>a rice ball for</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>sspring moonset --\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>spring moonset --\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>aa rice ball for\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>a rice ball for\\nb\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>bbreakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>breakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143132</th>\n",
       "      <td>I'm not asking did</td>\n",
       "      <td>you say it nor clarify</td>\n",
       "      <td>what you said neither</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>II'm not asking did\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>I'm not asking did\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>you say it nor clarify\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>you say it nor clarify\\nw\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>wwhat you said neither\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>what you said neither\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143133</th>\n",
       "      <td>You are truly a</td>\n",
       "      <td>moron or a liar I'm</td>\n",
       "      <td>inclined to think both</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>YYou are truly a\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>You are truly a\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>moron or a liar I'm\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>moron or a liar I'm\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>iinclined to think both\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>inclined to think both\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143134</th>\n",
       "      <td>Ain't no selfie on</td>\n",
       "      <td>this earth that's gonna make me</td>\n",
       "      <td>like Theresa May</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>AAin't no selfie on\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>Ain't no selfie on\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>this earth that's gonna make me\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>this earth that's gonna make me\\nl\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>llike Theresa May\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>like Theresa May\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143135</th>\n",
       "      <td>is doing a great</td>\n",
       "      <td>job turning Independents</td>\n",
       "      <td>into Democrats</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>iis doing a great\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>is doing a great\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>job turning Independents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>job turning Independents\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>iinto Democrats\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>into Democrats\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143136</th>\n",
       "      <td>Wanted to send a</td>\n",
       "      <td>quick follow up on if the</td>\n",
       "      <td>blood is loud Talk soon</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>WWanted to send a\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>Wanted to send a\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>quick follow up on if the\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>quick follow up on if the\\nb\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>bblood is loud Talk soon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>blood is loud Talk soon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170155 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0                                 1  \\\n",
       "0          Memorial Day --                 a shadow for each   \n",
       "1            spring rain -              as the doctor speaks   \n",
       "1            spring rain -              as the doctor speaks   \n",
       "2        spring moonset --                   a rice ball for   \n",
       "2        spring moonset --                   a rice ball for   \n",
       "...                    ...                               ...   \n",
       "143132  I'm not asking did            you say it nor clarify   \n",
       "143133     You are truly a               moron or a liar I'm   \n",
       "143134  Ain't no selfie on   this earth that's gonna make me   \n",
       "143135    is doing a great          job turning Independents   \n",
       "143136    Wanted to send a         quick follow up on if the   \n",
       "\n",
       "                              2 0_syllables 1_syllables 2_syllables  \\\n",
       "0                   white cross           5           5           2   \n",
       "1             i think of lilacs           2           5           5   \n",
       "1             i think of lilacs           3           5           5   \n",
       "2                     breakfast           3           4           2   \n",
       "2                     breakfast           4           4           2   \n",
       "...                         ...         ...         ...         ...   \n",
       "143132    what you said neither           5           7           5   \n",
       "143133   inclined to think both           5           7           5   \n",
       "143134         like Theresa May           5           7           5   \n",
       "143135           into Democrats           5           7           5   \n",
       "143136  blood is loud Talk soon           5           7           5   \n",
       "\n",
       "                                                     0_in  \\\n",
       "0       MMemorial Day --\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       sspring rain -\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       sspring rain -\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       sspring moonset --\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       sspring moonset --\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "...                                                   ...   \n",
       "143132  II'm not asking did\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143133  YYou are truly a\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143134  AAin't no selfie on\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143135  iis doing a great\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143136  WWanted to send a\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "\n",
       "                                                    0_out  \\\n",
       "0       Memorial Day --\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       spring rain -\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       spring rain -\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       spring moonset --\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       spring moonset --\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "...                                                   ...   \n",
       "143132  I'm not asking did\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143133  You are truly a\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143134  Ain't no selfie on\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143135  is doing a great\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143136  Wanted to send a\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "\n",
       "                                                     1_in  \\\n",
       "0       aa shadow for each\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       aas the doctor speaks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "1       aas the doctor speaks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "2       aa rice ball for\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       aa rice ball for\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "...                                                   ...   \n",
       "143132    you say it nor clarify\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143133    moron or a liar I'm\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143134    this earth that's gonna make me\\n\\n\\n\\n\\n\\n\\...   \n",
       "143135    job turning Independents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143136    quick follow up on if the\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "\n",
       "                                                    1_out  \\\n",
       "0       a shadow for each\\nw\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       as the doctor speaks\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "1       as the doctor speaks\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "2       a rice ball for\\nb\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       a rice ball for\\nb\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "...                                                   ...   \n",
       "143132   you say it nor clarify\\nw\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143133   moron or a liar I'm\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143134   this earth that's gonna make me\\nl\\n\\n\\n\\n\\n\\...   \n",
       "143135   job turning Independents\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143136   quick follow up on if the\\nb\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "\n",
       "                                                     2_in  \\\n",
       "0       wwhite cross\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       ii think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       ii think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       bbreakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       bbreakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "...                                                   ...   \n",
       "143132  wwhat you said neither\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143133  iinclined to think both\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143134  llike Theresa May\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143135  iinto Democrats\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143136  bblood is loud Talk soon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "\n",
       "                                                    2_out  \n",
       "0       white cross\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "1       i think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "1       i think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "2       breakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "2       breakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "...                                                   ...  \n",
       "143132  what you said neither\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "143133  inclined to think both\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "143134  like Theresa May\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "143135  into Democrats\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "143136  blood is loud Talk soon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "\n",
       "[170155 rows x 12 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad the lines to the max line length with new lines\n",
    "for i in range(3):\n",
    "    # For input, duplicate the first character\n",
    "    # TODO - Why?\n",
    "    df['%s_in' % i] = (df[str(i)].str[0] + df[str(i)]).str.pad(max_line_length+2, 'right', '\\n')\n",
    "    \n",
    "    # \n",
    "    #df['%s_out' % i] = df[str(i)].str.pad(max_line_len, 'right', '\\n') + ('\\n' if i == 2 else df[str(i+1)].str[0])\n",
    "    \n",
    "    # TODO - trying to add the next line's first character before the line breaks\n",
    "    if i == 2: # If it's the last line\n",
    "        df['%s_out' % i] = df[str(i)].str.pad(max_line_length+2, 'right', '\\n')\n",
    "    else: \n",
    "        # If it's the first or second line, add the first character of the next line to the end of this line.\n",
    "        # This helps with training so that the next RNN has a better chance of getting the first character right.\n",
    "        df['%s_out' % i] = (df[str(i)] + '\\n' + df[str(i+1)].str[0]).str.pad(max_line_length+2, 'right', '\\n')\n",
    "    \n",
    "max_line_length += 2\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df[['0_in', '1_in', '2_in']].values\n",
    "\n",
    "tokenizer = Tokenizer(filters='', char_level=True)\n",
    "tokenizer.fit_on_texts(inputs.flatten())\n",
    "n_tokens = len(tokenizer.word_counts) + 1\n",
    "\n",
    "# X is the input for each line in sequences of one-hot-encoded values\n",
    "X = to_categorical([\n",
    "    tokenizer.texts_to_sequences(inputs[:,i]) for i in range(3)\n",
    "], num_classes=n_tokens)\n",
    "\n",
    "outputs = df[['0_out', '1_out', '2_out']].values\n",
    "\n",
    "# Y is the output for each line in sequences of one-hot-encoded values\n",
    "Y = to_categorical([\n",
    "    tokenizer.texts_to_sequences(outputs[:,i]) for i in range(3)\n",
    "], num_classes=n_tokens)\n",
    "\n",
    "# X_syllables is the count of syllables for each line\n",
    "X_syllables = df[['0_syllables', '1_syllables', '2_syllables']].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process X's for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your raw text poems\n",
    "inputs[0, 0:3]\n",
    "\n",
    "# tokenized\n",
    "tks = tokenizer.texts_to_sequences(inputs[0,0:3])\n",
    "\n",
    "# tokenized --> categorical (1-hot encoded)\n",
    "to_categorical(tks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Y's for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MMemorial Day --\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "       'aa shadow for each\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "       'wwhite cross\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0, 0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Memorial Day --\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "       'a shadow for each\\nw\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "       'white cross\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "1. inputs[0, 0:3] --> contains 3 lines from a poem\n",
    "2. processed by tokenizer looks like this: [[43, 5, 7, 1, 2, 5, 7, 0, 0, 0, 0, 0] ... X3]\n",
    "3. becomes categorical (1 hot encoded) so [43] => [0, 0, 0, 0, 0, ... , 1, 0, 0, 0, 0]\n",
    "\n",
    "^ This process gives you your *X* & basically *Y*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X (syllables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['5', '5', '2'],\n",
       "       ['2', '5', '5'],\n",
       "       ['3', '5', '5'],\n",
       "       ...,\n",
       "       ['5', '7', '5'],\n",
       "       ['5', '7', '5'],\n",
       "       ['5', '7', '5']], dtype=object)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output_all_data_test_2\\\\metadata.pkl']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump([latent_dim, n_tokens, max_line_length, tokenizer], str(output_dir / 'metadata.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_model, lstm, lines, inputs, outputs = create_training_model(latent_dim, n_tokens)\n",
    "\n",
    "filepath = str(output_dir / (\"%s-{epoch:02d}-{loss:.2f}-{val_loss:.2f}.hdf5\" % latent_dim))\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "csv_logger = CSVLogger(str(output_dir / 'training_log.csv'), append=True, separator=',')\n",
    "\n",
    "callbacks_list = [checkpoint, csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'output_line_0/truediv:0' shape=(?, ?, 81) dtype=float32>,\n",
       " <tf.Tensor 'output_line_1/truediv:0' shape=(?, ?, 81) dtype=float32>,\n",
       " <tf.Tensor 'output_line_2/truediv:0' shape=(?, ?, 81) dtype=float32>]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create_training_model outputs...\n",
    "\n",
    "training_model # keras training model\n",
    "\n",
    "lstm # keras.layers --> LSTM layer\n",
    "\n",
    "lines # 3 - TrainingLine's\n",
    "\n",
    "#! Not USED\n",
    "inputs # 6 - Tensors...\n",
    "# char_input_line_1\n",
    "# syllable_input_line_1\n",
    "# ... 3x times\n",
    "\n",
    "outputs # 3 - Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 153139 samples, validate on 17016 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-92f59b00b6bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     ], epochs=epochs, validation_split=.1\n\u001b[0m\u001b[0;32m     15\u001b[0m )\n",
      "\u001b[1;32mc:\\Users\\brand\\Desktop\\Github Repos\\NLP\\haikurnn\\haiku-3.6\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\Users\\brand\\Desktop\\Github Repos\\NLP\\haikurnn\\haiku-3.6\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\brand\\Desktop\\Github Repos\\NLP\\haikurnn\\haiku-3.6\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 2986\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2988\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\brand\\Desktop\\Github Repos\\NLP\\haikurnn\\haiku-3.6\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_model.fit(\n",
    "    [\n",
    "        X[0],\n",
    "        X_syllables[:,0],\n",
    "        X[1],\n",
    "        X_syllables[:,1], \n",
    "        X[2],\n",
    "        X_syllables[:,2]\n",
    "    ],\n",
    "    [\n",
    "        Y[0],\n",
    "        Y[1],\n",
    "        Y[2]\n",
    "    ], epochs=epochs, validation_split=.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(lstm, lines, tf_session, tokenizer, n_tokens, max_line_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te  htecÂ—t se3\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-2827a2eb0fe6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# import importlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# importlib.reload(models)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_haiku\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\brand\\Desktop\\Github Repos\\NLP\\haikurnn\\notebooks\\models\\v1\\models.py\u001b[0m in \u001b[0;36mgenerate_haiku\u001b[1;34m(self, syllables, temperature, first_char)\u001b[0m\n\u001b[0;32m    151\u001b[0m                             [\n\u001b[0;32m    152\u001b[0m                                 to_categorical(\n\u001b[1;32m--> 153\u001b[1;33m                                     \u001b[0mline_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m                                 )\n\u001b[0;32m    155\u001b[0m                             ]\n",
      "\u001b[1;32mc:\\Users\\brand\\Desktop\\Github Repos\\NLP\\haikurnn\\haiku-3.6\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes)\u001b[0m\n\u001b[0;32m     37\u001b[0m       \u001b[0mlast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m   \"\"\"\n\u001b[1;32m---> 39\u001b[1;33m   \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m   \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "# import importlib\n",
    "# importlib.reload(models)\n",
    "generator.generate_haiku()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
